{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Motore di ricerca\n",
    "\n",
    "Basato su RAG"
   ],
   "id": "8deb60de2d785e6"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-25T10:47:50.931606Z",
     "start_time": "2025-02-25T10:47:13.674245Z"
    }
   },
   "source": [
    "from src.Generation import rag_pipeline\n",
    "\n",
    "pquery = \"Quali piatti contengono i ravioli al vaporeon?\"\n",
    "\n",
    "#response = search(\"../../HackapizzaDataset\", pquery, 20)\n",
    "response = rag_pipeline(\"HackapizzaDataset\", pquery,5)\n",
    "\n",
    "print(\"ğŸ”¹ RISPOSTA:\")\n",
    "print(response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ Caricando: HackapizzaDataset/Blogpost/blog_sapore_del_dune.html\n",
      "ğŸŒ Caricando: HackapizzaDataset/Blogpost/blog_etere_del_gusto.html\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Menu/Stelle Astrofisiche.pdf\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Menu/Ristorante delle Dune Stellari.pdf\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Menu/L Eco dei Sapori.pdf\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Menu/L Essenza delle Dune.pdf\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Menu/L Essenza del Multiverso su Pandora.pdf\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Menu/Armonia Universale.pdf\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Menu/Il Firmamento.pdf\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Menu/Universo Gastronomico di Namecc.pdf\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Menu/L Essenza di Asgard.pdf\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Menu/Tutti a TARSvola.pdf\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Menu/L Etere del Gusto.pdf\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Menu/L Universo in Cucina.pdf\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Menu/L Architetto dell Universo.pdf\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Menu/Ristorante Quantico.pdf\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Menu/Anima Cosmica.pdf\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Menu/L Equilibrio Quantico.pdf\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Menu/Essenza dell Infinito.pdf\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Menu/Stelle dell Infinito Celestiale.pdf\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Menu/L infinito in un Boccone.pdf\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Menu/Sala del Valhalla.pdf\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Menu/Le Stelle che Ballano.pdf\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Menu/Eco di Pandora.pdf\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Menu/Sapore del Dune.pdf\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Menu/L Oasi delle Dune Stellari.pdf\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Menu/Le Stelle Danzanti.pdf\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Menu/Datapizza.pdf\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Menu/Cosmica Essenza.pdf\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Menu/L Essenza Cosmica.pdf\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Menu/Le Dimensioni del Gusto.pdf\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Menu/Eredita Galattica.pdf\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Misc/Manuale di Cucina.pdf\n",
      "ğŸ“„ Caricando: HackapizzaDataset/Codice Galattico/Codice Galattico.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 2060 SUPER) - 5893 MiB free\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from models/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: PAD token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: layer   0 assigned to device CUDA0\n",
      "load_tensors: layer   1 assigned to device CUDA0\n",
      "load_tensors: layer   2 assigned to device CUDA0\n",
      "load_tensors: layer   3 assigned to device CUDA0\n",
      "load_tensors: layer   4 assigned to device CUDA0\n",
      "load_tensors: layer   5 assigned to device CUDA0\n",
      "load_tensors: layer   6 assigned to device CUDA0\n",
      "load_tensors: layer   7 assigned to device CUDA0\n",
      "load_tensors: layer   8 assigned to device CUDA0\n",
      "load_tensors: layer   9 assigned to device CUDA0\n",
      "load_tensors: layer  10 assigned to device CUDA0\n",
      "load_tensors: layer  11 assigned to device CUDA0\n",
      "load_tensors: layer  12 assigned to device CUDA0\n",
      "load_tensors: layer  13 assigned to device CUDA0\n",
      "load_tensors: layer  14 assigned to device CUDA0\n",
      "load_tensors: layer  15 assigned to device CUDA0\n",
      "load_tensors: layer  16 assigned to device CUDA0\n",
      "load_tensors: layer  17 assigned to device CUDA0\n",
      "load_tensors: layer  18 assigned to device CUDA0\n",
      "load_tensors: layer  19 assigned to device CUDA0\n",
      "load_tensors: layer  20 assigned to device CUDA0\n",
      "load_tensors: layer  21 assigned to device CUDA0\n",
      "load_tensors: layer  22 assigned to device CUDA0\n",
      "load_tensors: layer  23 assigned to device CUDA0\n",
      "load_tensors: layer  24 assigned to device CUDA0\n",
      "load_tensors: layer  25 assigned to device CUDA0\n",
      "load_tensors: layer  26 assigned to device CUDA0\n",
      "load_tensors: layer  27 assigned to device CUDA0\n",
      "load_tensors: layer  28 assigned to device CUDA0\n",
      "load_tensors: layer  29 assigned to device CUDA0\n",
      "load_tensors: layer  30 assigned to device CUDA0\n",
      "load_tensors: layer  31 assigned to device CUDA0\n",
      "load_tensors: layer  32 assigned to device CUDA0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors: offloading 32 repeating layers to GPU\n",
      "load_tensors: offloading output layer to GPU\n",
      "load_tensors: offloaded 33/33 layers to GPU\n",
      "load_tensors:        CUDA0 model buffer size =  4095.05 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =    70.31 MiB\n",
      "llama_init_from_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 8192\n",
      "llama_init_from_model: n_ctx_per_seq = 8192\n",
      "llama_init_from_model: n_batch       = 32\n",
      "llama_init_from_model: n_ubatch      = 8\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\n",
      "llama_init_from_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_init_from_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:      CUDA0 compute buffer size =     9.50 MiB\n",
      "llama_init_from_model:  CUDA_Host compute buffer size =     1.13 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 2\n",
      "CUDA : ARCHS = 750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ Generazione della risposta in corso...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   23098.81 ms\n",
      "llama_perf_context_print: prompt eval time =   23098.69 ms /  3145 tokens (    7.34 ms per token,   136.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7713.83 ms /   299 runs   (   25.80 ms per token,    38.76 tokens per second)\n",
      "llama_perf_context_print:       total time =   31079.42 ms /  3444 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ RISPOSTA:\n",
      "\n",
      "    Risposta: I ravioli al vaporeon sono un piatto tradizionale di cucina stellare. Essi si presentano come ravioli ripieni di una miscela di formaggio, spezie e polvere di Stelle. Il ripieno Ã¨ avvolto da una sottile panna di mandorle, che conferisce al piatto una leggerissima sensazione di frangere un velo di cristallo. Quindi, i ravioli vengono cotti al vaporeon, ossia in un'infusione di acqua e foglie di Nebulosa. La cotura al vaporeon conferisce alle pastine una consistenza morbida e una forma che ricorda le goccioline di luna. Infine, i ravioli al vaporeon possono essere serviti come piatto principale insieme a una salsa di baccacedro o foglie di Mandragora.\n",
      "    \n",
      "     Note: This response is based on the assumption that the term \"ravioli al vaporeon\" refers to a specific dish or ingredient in the context of cucina stellare. If this is not the case, then the information provided may not be accurate or relevant. In any event, it is always important to double-check any information before relying on it or sharing it with others.\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
