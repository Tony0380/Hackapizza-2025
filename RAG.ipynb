{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Motore di ricerca\n",
    "\n",
    "Basato su RAG"
   ],
   "id": "8deb60de2d785e6"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-25T09:49:43.684584Z",
     "start_time": "2025-02-25T09:47:02.941276Z"
    }
   },
   "source": [
    "from src.Generation import rag_pipeline\n",
    "\n",
    "pquery = \"Quali sono i piatti che includono i Sashimi di Magikarp?\"\n",
    "\n",
    "#response = search(\"../../HackapizzaDataset\", pquery, 20)\n",
    "response = rag_pipeline(\"HackapizzaDataset\", pquery,10)\n",
    "\n",
    "print(\"üîπ RISPOSTA:\")\n",
    "print(response)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tony/Repository/Hackapizza-2025/src/Retrieval/text_processing.py:36: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç Caricando: HackapizzaDataset/Blogpost/blog_sapore_del_dune.html\n",
      "üåç Caricando: HackapizzaDataset/Blogpost/blog_etere_del_gusto.html\n",
      "üìÑ Caricando: HackapizzaDataset/Menu/Stelle Astrofisiche.pdf\n",
      "üìÑ Caricando: HackapizzaDataset/Menu/Ristorante delle Dune Stellari.pdf\n",
      "üìÑ Caricando: HackapizzaDataset/Menu/L Eco dei Sapori.pdf\n",
      "üìÑ Caricando: HackapizzaDataset/Menu/L Essenza delle Dune.pdf\n",
      "üìÑ Caricando: HackapizzaDataset/Menu/L Essenza del Multiverso su Pandora.pdf\n",
      "üìÑ Caricando: HackapizzaDataset/Menu/Armonia Universale.pdf\n",
      "üìÑ Caricando: HackapizzaDataset/Menu/Il Firmamento.pdf\n",
      "üìÑ Caricando: HackapizzaDataset/Menu/Universo Gastronomico di Namecc.pdf\n",
      "üìÑ Caricando: HackapizzaDataset/Menu/L Essenza di Asgard.pdf\n",
      "üìÑ Caricando: HackapizzaDataset/Menu/Tutti a TARSvola.pdf\n",
      "üìÑ Caricando: HackapizzaDataset/Menu/L Etere del Gusto.pdf\n",
      "üìÑ Caricando: HackapizzaDataset/Menu/L Universo in Cucina.pdf\n",
      "üìÑ Caricando: HackapizzaDataset/Menu/L Architetto dell Universo.pdf\n",
      "üìÑ Caricando: HackapizzaDataset/Menu/Ristorante Quantico.pdf\n",
      "üìÑ Caricando: HackapizzaDataset/Menu/Anima Cosmica.pdf\n",
      "üìÑ Caricando: HackapizzaDataset/Menu/L Equilibrio Quantico.pdf\n",
      "üìÑ Caricando: HackapizzaDataset/Menu/Essenza dell Infinito.pdf\n",
      "üìÑ Caricando: HackapizzaDataset/Menu/Stelle dell Infinito Celestiale.pdf\n",
      "üìÑ Caricando: HackapizzaDataset/Menu/L infinito in un Boccone.pdf\n",
      "üìÑ Caricando: HackapizzaDataset/Menu/Sala del Valhalla.pdf\n",
      "üìÑ Caricando: HackapizzaDataset/Menu/Le Stelle che Ballano.pdf\n",
      "üìÑ Caricando: HackapizzaDataset/Menu/Eco di Pandora.pdf\n",
      "üìÑ Caricando: HackapizzaDataset/Menu/Sapore del Dune.pdf\n",
      "üìÑ Caricando: HackapizzaDataset/Menu/L Oasi delle Dune Stellari.pdf\n",
      "üìÑ Caricando: HackapizzaDataset/Menu/Le Stelle Danzanti.pdf\n",
      "üìÑ Caricando: HackapizzaDataset/Menu/Datapizza.pdf\n",
      "üìÑ Caricando: HackapizzaDataset/Menu/Cosmica Essenza.pdf\n",
      "üìÑ Caricando: HackapizzaDataset/Menu/L Essenza Cosmica.pdf\n",
      "üìÑ Caricando: HackapizzaDataset/Menu/Le Dimensioni del Gusto.pdf\n",
      "üìÑ Caricando: HackapizzaDataset/Menu/Eredita Galattica.pdf\n",
      "üìÑ Caricando: HackapizzaDataset/Misc/Manuale di Cucina.pdf\n",
      "üìÑ Caricando: HackapizzaDataset/Codice Galattico/Codice Galattico.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 2060 SUPER, compute capability 7.5, VMM: yes\n",
      "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 2060 SUPER) - 6359 MiB free\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from models/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: PAD token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: layer   0 assigned to device CUDA0\n",
      "load_tensors: layer   1 assigned to device CUDA0\n",
      "load_tensors: layer   2 assigned to device CUDA0\n",
      "load_tensors: layer   3 assigned to device CUDA0\n",
      "load_tensors: layer   4 assigned to device CUDA0\n",
      "load_tensors: layer   5 assigned to device CUDA0\n",
      "load_tensors: layer   6 assigned to device CUDA0\n",
      "load_tensors: layer   7 assigned to device CUDA0\n",
      "load_tensors: layer   8 assigned to device CUDA0\n",
      "load_tensors: layer   9 assigned to device CUDA0\n",
      "load_tensors: layer  10 assigned to device CUDA0\n",
      "load_tensors: layer  11 assigned to device CUDA0\n",
      "load_tensors: layer  12 assigned to device CUDA0\n",
      "load_tensors: layer  13 assigned to device CUDA0\n",
      "load_tensors: layer  14 assigned to device CUDA0\n",
      "load_tensors: layer  15 assigned to device CUDA0\n",
      "load_tensors: layer  16 assigned to device CUDA0\n",
      "load_tensors: layer  17 assigned to device CUDA0\n",
      "load_tensors: layer  18 assigned to device CUDA0\n",
      "load_tensors: layer  19 assigned to device CUDA0\n",
      "load_tensors: layer  20 assigned to device CUDA0\n",
      "load_tensors: layer  21 assigned to device CUDA0\n",
      "load_tensors: layer  22 assigned to device CUDA0\n",
      "load_tensors: layer  23 assigned to device CUDA0\n",
      "load_tensors: layer  24 assigned to device CUDA0\n",
      "load_tensors: layer  25 assigned to device CUDA0\n",
      "load_tensors: layer  26 assigned to device CUDA0\n",
      "load_tensors: layer  27 assigned to device CUDA0\n",
      "load_tensors: layer  28 assigned to device CUDA0\n",
      "load_tensors: layer  29 assigned to device CUDA0\n",
      "load_tensors: layer  30 assigned to device CUDA0\n",
      "load_tensors: layer  31 assigned to device CUDA0\n",
      "load_tensors: layer  32 assigned to device CUDA0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors: offloading 32 repeating layers to GPU\n",
      "load_tensors: offloading output layer to GPU\n",
      "load_tensors: offloaded 33/33 layers to GPU\n",
      "load_tensors:        CUDA0 model buffer size =  4095.05 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =    70.31 MiB\n",
      "llama_init_from_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 16384\n",
      "llama_init_from_model: n_ctx_per_seq = 16384\n",
      "llama_init_from_model: n_batch       = 32\n",
      "llama_init_from_model: n_ubatch      = 8\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (16384) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 16384, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB\n",
      "llama_init_from_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_init_from_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:      CUDA0 compute buffer size =    18.50 MiB\n",
      "llama_init_from_model:  CUDA_Host compute buffer size =     2.13 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 2\n",
      "CUDA : ARCHS = 750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Generazione della risposta in corso...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  100701.81 ms\n",
      "llama_perf_context_print: prompt eval time =  100701.62 ms / 10346 tokens (    9.73 ms per token,   102.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =   48313.61 ms /  1023 runs   (   47.23 ms per token,    21.17 tokens per second)\n",
      "llama_perf_context_print:       total time =  150887.19 ms / 11369 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ RISPOSTA:\n",
      " unassggiscor unan unaccoslego chezza avaggassiaana,la ilet lezzaggantezzass eccimosz ilarogolinaviacoletsteriegiggis unadovitegga insanogialepog in ilesteramente la ecorolocunio golsterogster-mentezzilac unazaca a risor,aghozza, a lassai chelolineggi aze alt unase- uninfollessa legacel ilancelyolabbologle unampan lantile ilalovazzolavalaa inssa unasterana stun unall unacca aster corin offa e ile unoce questacasa una volaicanaa la sfogos ilocil ila il√† ilogiagaggzzagolsterzzale ilacsson ileganoligtessaacol ila ila unaranolovsole aa inssle-agga rigantela unosanazza uno unare unalla unassvfriellla laffl ileg unare inlaggi√®le esventatolasterimia ila sopinol a una volacclaggely uninfiluna coraun unaiaunbr unaq ezztog a lava uncoronla gla,dazssly unao olt unabb uncollole untezza inciresolle ileglaggia unase ilavormenteolegoviololavilun unalagi,onanazzalegganananlebragganogimol a alavilcebren unamialoleggis unalla in unare insimmela ingim ilinfansamente scattelylava risaclalagule unancantevolaca occa ila ila ilelcor sor corinfass eslleggogizzasscorcorle unadine ila ilsun unc ila ilantonil a aletggiogmentegglela lazilre ilsolovaclox rissocin chegg chegola eviamente riscuqu unares corim unan sucan ilattoruoo unain una voliosc unar una scer unafera in unad a unassmenteggiale a rie aunla ilar ganc la-ion unaggi avince corde regim una ovainunun cora di esvaggia sogac afran sulaost che unalazza ils ils siv dirove unampa unamboollof ilassania ebba unaderte unle lad unoletcorne diaccamente unass ilegoge unambil unagmanunaggiare unavava lavaggla coraman unaggunicagil ilarimmente rivest unavol laggolavazza lav ila\n",
      "man un'\n",
      " a rigol risanlabramanacor unaigigana unall unaffa unassozz uncolantonteggmentei e iacolin uninfogil chezza-man unabba lazzaan unadun ila unazza la unavi allavenana ila unog risa ila ileg ilcla uncezza ila alalole ilacun ilazzancormenteggia unaccimman unagu unavelaziaa\n",
      "man ilararacacoronnaa ilassnman lareman unaro che chell ecc chett unariecci concana gogzzazzassla una insara unva che inson e scor sulo a ila unana una volaa ozza unagg unafia ila la uncorna tra a alla unavil unale ila una e ila il'manz ilaril la ilonggoos unagg unavolgman\n",
      "ss un al chegmanla unavaggflicmentegg.oosgg.zza unagganacor una sac unazzasterun un unav ilo ilsc rig una ilala una ilgn iloogo il illezza che una ilgg inmanman unagg ilss a a una rifacc ungg,man ris\n",
      "a eggizzgmanmanmenteccun una la il g legg unaggzza socolor s unazza una scattla,,iamango corbaa a es un ila,achezza ilar una\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
