{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# RAG: Retrieval-Augmented Generation\n",
   "id": "c30e5eb98863b7c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Caricamento di documenti PDF\n",
    "\n",
    "## Funzionamento:\n",
    "1. Si importa la funzione `load_pdfs_recursively` dal modulo `document_loaders` di LangChain.\n",
    "2. Si definisce la cartella contenente i documenti PDF.\n",
    "3. Si invoca la funzione `load_pdfs_recursively` passando il percorso della cartella come argomento.\n",
    "4. La funzione carica ricorsivamente tutti i file PDF da quella cartella e dalle sue sottocartelle.\n",
    "5. I documenti vengono restituiti come una lista di oggetti `Document`."
   ],
   "id": "be0b087a65e6d466"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "def load_pdfs_recursively(folder_path):\n",
    "    \"\"\"\n",
    "    Carica ricorsivamente tutti i file PDF da una cartella e dalle sue sottocartelle.\n",
    "\n",
    "    :param folder_path: Percorso della cartella principale.\n",
    "    :return: Lista di documenti caricati.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "\n",
    "    # Scansiona la cartella e sottocartelle\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for filename in files:\n",
    "            print(f\"üìÅ Scansionando: {root}\")\n",
    "            if filename.endswith(\".pdf\"):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                print(f\"üìÑ Caricando: {file_path}\")\n",
    "\n",
    "                # Carica il PDF\n",
    "                loader = PyMuPDFLoader(file_path)\n",
    "                documents.extend(loader.load())  # Aggiunge i documenti alla lista\n",
    "\n",
    "    print(f\"‚úÖ Caricati {len(documents)} documenti da {folder_path}\")\n",
    "    return documents\n",
    "\n",
    "dataset_path = \"../HackapizzaDataset/\"\n",
    "all_pdfs = load_pdfs_recursively(dataset_path)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Caricamento di documenti HTML\n",
    "\n",
    "## Funzionamento:\n",
    "1. Si importa la funzione `load_htmls_recursively` dal modulo `document_loaders` di LangChain.\n",
    "2. Si definisce la cartella contenente i documenti HTML.\n",
    "3. Si invoca la funzione `load_htmls_recursively` passando il percorso della cartella come argomento.\n",
    "4. La funzione carica ricorsivamente tutti i file HTML da quella cartella e dalle sue sottocartelle.\n",
    "5. I documenti vengono restituiti come una lista di oggetti `Document`."
   ],
   "id": "1fc6bf6736d3273c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from langchain.document_loaders import BSHTMLLoader\n",
    "\n",
    "def load_htmls_recursively(folder_path):\n",
    "    \"\"\"\n",
    "    Carica ricorsivamente tutti i file HTML da una cartella e dalle sue sottocartelle.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for filename in files:\n",
    "            if filename.lower().endswith((\".html\", \".htm\")):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                print(f\"üåç Caricando: {file_path}\")\n",
    "\n",
    "                try:\n",
    "                    # Forza l'uso di html.parser se lxml non funziona\n",
    "                    loader = BSHTMLLoader(file_path, bs_kwargs={\"features\": \"html.parser\"})\n",
    "                    documents.extend(loader.load())\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Errore con {file_path}: {e}\")\n",
    "\n",
    "    print(f\"‚úÖ Caricati {len(documents)} documenti HTML da {folder_path}\")\n",
    "    return documents\n",
    "\n",
    "dataset_path = \"../HackapizzaDataset/\"\n",
    "all_htmls = load_htmls_recursively(dataset_path)"
   ],
   "id": "e11864cf1ba12678",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Unione di documenti PDF e HTML\n",
    "\n"
   ],
   "id": "41f233a8f14a30fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def loadDocuments(folder_path):\n",
    "    documents = load_htmls_recursively(folder_path)\n",
    "    documents.extend(load_pdfs_recursively(folder_path))\n",
    "    return documents\n",
    "\n",
    "documents = loadDocuments(\"../HackapizzaDataset/\")\n",
    "print(f\"üìÑ Documenti caricati: {documents}\")  # Debug"
   ],
   "id": "bdfebc495a33699e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Chunking di documenti\n",
    "\n",
    "## Funzionamento:\n",
    "Chuking a dimensione fissa di documenti."
   ],
   "id": "2347b52026d7dd13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "documents = loadDocuments(\"../HackapizzaDataset/\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(documents)\n"
   ],
   "id": "44fc61076ba8f7db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Embedding di chunk\n",
    "\n",
    "## Funzionamento:\n",
    "Embedding di chunk di testo utilizzando un modello preaddestrato."
   ],
   "id": "3e1e1b0968fabfb9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Inizializza il modello di embedding\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Genera gli embedding per i chunk di testo\n",
    "chunk_embeddings = embedding_model.embed_documents([chunk.page_content for chunk in chunks])"
   ],
   "id": "753384bcd63c7a47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Indicizzamento di chuck in un vettore store\n",
    "\n",
    "## FAISS"
   ],
   "id": "d4395d3116111aae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Inizializza FAISS con gli embedding e i metadati dei chunk\n",
    "vectorstore = FAISS.from_texts(\n",
    "    texts=[chunk.page_content for chunk in chunks],\n",
    "    embedding=embedding_model\n",
    ")"
   ],
   "id": "78be4d36c03e210c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Motore di ricerca\n",
    "\n",
    "## Classificatore di tipo KNN\n"
   ],
   "id": "e207f49f01890cb9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "query = (\"chef Aurora Stellaris\")  # Query di ricerca\n",
    "results = vectorstore.similarity_search(query, k=5)  # Trova i 5 documenti pi√π simili\n",
    "\n",
    "# Stampa i risultati\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"üîπ Risultato {i+1}: {doc.page_content[:500]}...\\n\")\n"
   ],
   "id": "9260998036e8202f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
